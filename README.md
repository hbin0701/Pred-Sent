# Let's Predict Sentence by Sentence

> âš ï¸ **Work in Progress** Â â€”Â  The codebase is under active development. Will be updated soon.

This repository contains the *official* implementation of the paper **â€œLetâ€™sÂ PredictÂ SentenceÂ byÂ Sentence."**

ðŸ“ **TL;DR**: We present a framework that **adapts** a pre-trained token-level LM to operate in sentence space, by autoregressively predicting
continuous embeddings of next sentences.

## Directory Structure

```text
pred-sent/
â”œâ”€â”€ data/                     # Preâ€‘packed benchmark datasets
â”‚   â”œâ”€â”€ blocksworld7/
â”‚   â”œâ”€â”€ csqa/
â”‚   â”œâ”€â”€ gsm8k/
â”‚   â””â”€â”€ prosqa/
â”‚   â””â”€â”€ *.json
â””â”€â”€ src/                      # All training & inference pipelines
    â”œâ”€â”€ autoreg_ctx/          # Contextualâ€‘embedding latent model
    â”œâ”€â”€ autoreg_ctx_w_classifier/   # + termination classifier
    â”œâ”€â”€ autoreg_sem/          # Semanticâ€‘embedding latent model
    â”œâ”€â”€ emb_ctx/              # Contextual embedding training
    â”œâ”€â”€ emb_sem/              # Semantic embedding training
    â””â”€â”€ sft/                  # CoT & Noâ€‘CoT supervised baselines
```

Each subdirectory contains its own **`train.py`**, **`main.py`**, and **`run.sh`** that can be invoked directly. See teh bash script for more details.

## Model Architecture

Our overall models consist of three principal components 

| Component      | Paper name     | Purpose                                            |
| -------------- | -------------- | -------------------------------------------------- |
| **Encoder**    | *Encoder*      | Convert input query into a vector representation   |
| **Decoder**    | *LatentÂ Model* | Autoregressively emit intermediate reasoning steps |
| **Translator** | *Decoder*      | Map the emitted steps to the final answer          |

> *Naming note*: we are standardizing labels in code to be consistent with paper. Will be updated soon.

## Installation

### 1. Clone & Prepare Environment

```bash
# Clone repository
$ git clone https://github.com/hbin0701/pred-sent.git
$ cd pred-sent

# Create Conda environment (PyTorchÂ 2.3 + CUDAÂ 12 by default)
$ conda env create -f environment.yml
$ conda activate pred-sent
```

### 2. Download Checkpoints (optional)

TBD. We will enable this via:

```bash
$ bash scripts/download_data.sh  # ~5Â minutes / <1Â GB
```
